{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### 시작! : 전처리를 해보자"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Admin\\Anaconda3\\lib\\site-packages\\gensim\\utils.py:1197: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n",
      "C:\\Users\\Admin\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from gensim.models.word2vec import Word2Vec\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "import time\n",
    "import numpy as np\n",
    "import operator\n",
    "from konlpy.tag import Komoran\n",
    "from urllib.parse import quote_plus\n",
    "import lxml.html\n",
    "import pickle\n",
    "import re\n",
    "import keras\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## pickle load 하기\n",
    "with open('win_apply.txt', 'rb') as f:\n",
    "    win_apply = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"‘무임승차하지 않는 삶’ 중학 시절, 은사님께서 해주신 말씀입니다. 직접 노력해서 결과를 얻어야 한다는 기본적 원칙은 곧 제 인생의 가치관이 되었습니다. 인생의 가치관이 생기자 새로운 목표가 생겼습니다. 그 계기는 고등학생 시절 학창시절 우연히 하게 된 일용직 근로자 경험에서 시작되었습니다. 작은 벽돌이 쌓여서 큰 건물이 완성되는 모습은 저에겐 큰 놀라움이었습니다. 당시 어린 눈으로 봤던 건설현장은 저의 가치관이 그대로 투영된 곳으로 다가왔고 곧, 건설현장은 저에게 목표가 되었습니다. 저는 건축엔지니어로서의 기본을 다지기 위해 아래와 같은 노력을 했습니다. 다양한 분야의 지식을 습득했습니다. 건축은 구조, 시공, 안전 등 다양한 분야로 이루어져 있습니다. 그 때문에 한 가지 분야만 능통해서는 온전한 건물을 지을 수 없습니다. 그래서 저는 설계, 안전, 환경, 시공 등 다양한 전공수업을 이수하였습니다. 다양한 변수가 있는 현장에서 습득한 지식을 토대로 공기단축에 이바지하도록 하겠습니다. 다양한 경험을 위해 경진대회에 참여했습니다. 학부 시절 시공경진 대회에 참여했습니다. 대회를 준비하는 과정에서의 공부는 시공과 하자에 대한 다양한 지식을 얻을 수 있었습니다. 또한, 21개 팀이 참가한 경진대회에서 100명이 넘는 인원과 충남지역 대기업 실무자분들 앞에서 발표한 경험은 저를 더욱 단단하게 만들어 주었고, '대상'이라는 성과도 얻을 수 있었습니다.\",\n",
       " '가장 기억에 남는 전공은 안전공학입니다. 현직에 종사하시는 교수님 아래에서 안전의 이론과 더불어 현장에서의 적용을 동시에 배울 수 있었습니다. 안전교육법, 감성접근법 등을 배움은 분명 저의 강점이 될 것입니다. 이 수업을 통해 안전은 사람의 생명뿐만 아니라 공사의 성패와도 관련 있음을 알게 되었습니다. 안전이 현장에서 ‘필수’임을 깨달은 저는 건설안전기사를 취득하였습니다. 무재해 공사를 만들어 업계순위 10위 도약, 수주 10조 달성 등 비전 2020 달성에 앞장서겠습니다. 교양 과목에서 가장 관심 있었던 과목은 창의 발명과 지식재산의 이해입니다. 교수님은 ‘창의적 생각을 위해 비판하지 말고 수용하라’고 말씀하시며 ‘브레인스토밍’을 강조하셨습니다. 개인의 능력이 아닌 속한 공동체의 분위기가 창의적 생각의 중요한 부분임을 알게 되었습니다. 저는 이를 곧 제가 속한 조직에 적용하였고 학교와 교회 등에서 좋은 결과를 얻을 수 있었습니다. 조직에서 ‘개인’을 중요시하지 않고 인화 단결하여 창의적 해결방안을 제시하겠습니다.']"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "win_apply[0:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# sentence 만들기\n",
    "sentence = []\n",
    "for i in range(0, len(win_apply)):\n",
    "    a = win_apply[i].split(\".\")\n",
    "    for j in range(0, len(a)):\n",
    "        b = a[j]\n",
    "        sentence.append(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## 명사추출\n",
    "tagger = Komoran()\n",
    "\n",
    "def get_noun(text):\n",
    "    nouns = tagger.nouns(text)\n",
    "    return [n for n in nouns if len(n) >= 2]  # 명사만 추출\n",
    "\n",
    "words = []\n",
    "for i in range(0, len(sentence)):\n",
    "    a = get_noun(sentence[i])\n",
    "    words.append(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "문장수 : 208550\n",
      "명사 추출 단어 수 : 208550\n"
     ]
    }
   ],
   "source": [
    "print(\"문장수 :\",len(sentence))\n",
    "print(\"명사 추출 단어 수 :\",len(words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "값이 없는 리시트 삭제 후\n",
      "문장수 : 187492\n",
      "명사 추출 단어 수 : 187492\n",
      "\n",
      "\n",
      "명사 과다 추출된 리스트 삭제 후\n",
      "문장수 : 187400\n",
      "명사 추출 단어 수 : 187400\n"
     ]
    }
   ],
   "source": [
    "# 전처리\n",
    "none_index = []\n",
    "for i in range(0, len(words)):\n",
    "    if len(words[i]) == 0:\n",
    "        none_index.append(i)\n",
    "\n",
    "none_index.reverse()\n",
    "        \n",
    "for j in none_index:\n",
    "    del sentence[j]\n",
    "    del words[j]\n",
    "\n",
    "print(\"값이 없는 리시트 삭제 후\")\n",
    "print(\"문장수 :\",len(sentence))\n",
    "print(\"명사 추출 단어 수 :\",len(words))\n",
    "\n",
    "max_index = []\n",
    "for k in range(0, len(words)):\n",
    "    if len(words[k]) > 42:\n",
    "        max_index.append(k)\n",
    "\n",
    "max_index.reverse()\n",
    "\n",
    "for l in max_index:\n",
    "    del words[l]\n",
    "    del sentence[l]\n",
    "\n",
    "print(\"\\n\")\n",
    "print(\"명사 과다 추출된 리스트 삭제 후\")\n",
    "print(\"문장수 :\",len(sentence))\n",
    "print(\"명사 추출 단어 수 :\",len(words))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### WORD - 임베딩 벡터를 만들어보자"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word2Vec(vocab=23845, size=200, alpha=0.025)\n"
     ]
    }
   ],
   "source": [
    "# embedding\n",
    "embedding_model = Word2Vec(words,\n",
    "                           size=200,  # 100차원의 벡터로 바꾼다\n",
    "                           window = 2,  # 주변 단어 앞 뒤로 두개까지\n",
    "                           min_count = 1,  # 출현 빈도의 최소값, 나머진 삭제\n",
    "                           workers = 4,  # CPU는 쿼드코어를 사용\n",
    "                           iter = 100, # 100번 학습\n",
    "                           sg = 1 # 분석방법론은 CBOW와 Skip-Gram 중 후자 선택\n",
    "                          )\n",
    "print(embedding_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('전액', 0.4947859048843384), ('문중', 0.4554343521595001), ('필진', 0.4424971342086792), ('차석', 0.430867075920105), ('주립 대학', 0.4147442877292633), ('국가장학금', 0.41453737020492554), ('기독교 선교단체', 0.41203415393829346), ('상패', 0.41156190633773804), ('단원고등학교', 0.4099908769130707), ('토플', 0.4084552824497223)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/venv/lib/python3.6/site-packages/ipykernel_launcher.py:3: DeprecationWarning: Call to deprecated `most_similar` (Method will be removed in 4.0.0, use self.wv.most_similar() instead).\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    }
   ],
   "source": [
    "# 결과 확인\n",
    "print(embedding_model.most_similar(positive=[\"장학금\"],\n",
    "                                   topn=10) # 단어와 가장 비슷한(유사도가 큰) 100개 단어 출력\n",
    "     )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Image 벡터 처럼 키워드를 임베딩 벡터로 바꾸어 보장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# words에서 키워드 하나만 살리기\n",
    "for i in range(0, len(words)):\n",
    "    words[i] = words[i][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['무임승차', '노력', '인생', '계기', '벽돌']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function BufferedWriter.close>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# pickle에 저장하기\n",
    "# with open('words_1_dic.txt', 'wb') as f:\n",
    "#     pickle.dump(words, f)\n",
    "# f.close"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function BufferedReader.close>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 데이터 로드하기\n",
    "with open('words.txt', 'rb') as f:\n",
    "    words= pickle.load(f)\n",
    "f.close"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/venv/lib/python3.6/site-packages/ipykernel_launcher.py:3: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    }
   ],
   "source": [
    "# 워드 벡터 만들기\n",
    "for i in range(0, len(words)):\n",
    "    words[i] = embedding_model[words[i]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function BufferedWriter.close>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# pickle에 저장하기\n",
    "# with open('words_vec_200.txt', 'wb') as f:\n",
    "#     pickle.dump(words, f)\n",
    "# f.close"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function BufferedReader.close>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('words_vec_200.txt', 'rb') as f:\n",
    "    words = pickle.load(f)\n",
    "f.close"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function BufferedWriter.close>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('sentence.txt', 'wb') as f:\n",
    "    pickle.dump(sentence, f)\n",
    "f.close"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "187400\n",
      "187400\n"
     ]
    }
   ],
   "source": [
    "# 완성~\n",
    "print(len(words))\n",
    "print(len(sentence))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 문단 전처리를 합시다!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## pickle load 하기\n",
    "with open('win_apply.txt', 'rb') as f:\n",
    "    para = pickle.load(f)\n",
    "    \n",
    "# 문단에 대해 명사를 추출\n",
    "tagger = Komoran()\n",
    "\n",
    "def get_noun(text):\n",
    "    nouns = tagger.nouns(text)\n",
    "    return [n for n in nouns if len(n) >= 2]  # 명사만 추출\n",
    "\n",
    "words = []\n",
    "for i in range(0, len(para)):\n",
    "    a = get_noun(para[i])\n",
    "    words.append(a)\n",
    "\n",
    " # words 추출된 단어 개수 확인\n",
    "words_count = []\n",
    "for i in range(0,len(words)):\n",
    "    words_count.append(len(words[i]))\n",
    "\n",
    "# 인덱스 화\n",
    "words_keyword_dic = {w: i for i, w in enumerate(words_count)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18605\n",
      "18605\n"
     ]
    }
   ],
   "source": [
    "print(len(words))\n",
    "print(len(para))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "1. 300자 이하인 자소서를 제거\n",
    "2. 모든 명사를 추출 및 확인\n",
    "3. word2vec을 통해서 벡터화\n",
    "4. padding을 통해서 맞춰주기 → 함수 만들기\n",
    "5. 모델 돌리기\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/venv/lib/python3.6/site-packages/ipykernel_launcher.py:47: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "## 임베딩 벡터 만들기 ## \n",
    "데이터 전처리\n",
    "words : 명사 추출 데이터\n",
    "para : 문항 데이터\n",
    "para_length : 문항의 최소 글자 수\n",
    "\"\"\"\n",
    "\n",
    "para_length = 400\n",
    "\n",
    "index_over = []\n",
    "for i in range(0, len(para)):\n",
    "    if len(para[i]) < para_length:\n",
    "        index_over.append(i)\n",
    "    \n",
    "index_over.reverse() # 역순으로\n",
    "    \n",
    "# index_over 값을 데이터에서 제거\n",
    "for i in index_over:\n",
    "    del words[i], para[i]\n",
    "    \n",
    "# padding을 위해서(나머지 자리 0으로 채워주기 위해서) 단어의 최고 자리 구하기\n",
    "# words 추출된 단어 개수 확인\n",
    "words_count = []\n",
    "for i in range(0,len(words)):\n",
    "    words_count.append(len(words[i]))\n",
    "    \n",
    "words_length_max = max(words_count) # max_length 설정\n",
    "    \n",
    "# Words Word2Vec\n",
    "embedding_model = Word2Vec(words,\n",
    "                           size = 100,  # 200차원의 벡터로 바꾼다\n",
    "                           window = 2,  # 주변 단어 앞 뒤로 두개까지\n",
    "                           min_count = 1,  # 출현 빈도의 최소값, 나머진 삭제\n",
    "                           workers = 4,  # CPU는 쿼드코어를 사용\n",
    "                           iter = 100, # 100번 학습\n",
    "                           sg = 1 # 분석방법론은 CBOW와 Skip-Gram 중 후자 선택\n",
    "                          )    \n",
    "\n",
    "\n",
    "# 단어를 벡터 값으로 바꾸기\n",
    "words_vec = []\n",
    "for i in range(0, len(words)):\n",
    "    words_vec_before = []\n",
    "    \n",
    "    for j in range(0, len(words[i])):\n",
    "        a = embedding_model[words[i][j]]\n",
    "        for k in range(0, len(a)):\n",
    "            words_vec_before.append(a[k])\n",
    "                \n",
    "    words_vec.append(words_vec_before)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14918\n",
      "19200\n"
     ]
    }
   ],
   "source": [
    "print(len(words_vec))\n",
    "print(len(words_vec[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "dense vector shape fittning\n",
    "벡터의 shape 맞추기! = padding : 0으로 채우자\n",
    "\"\"\"\n",
    "words_vec_length_count = []\n",
    "for i in range(0, len(words_vec)):\n",
    "    words_vec_length_count.append(len(words_vec[i]))\n",
    "\n",
    "vec_max_length = max(words_vec_length_count) # \n",
    "\n",
    "for i in range(0, len(words_vec)):\n",
    "    if len(words_vec[i]) != vec_max_length:\n",
    "        for j in range(0, (vec_max_length-len(words_vec[i]))):\n",
    "            words_vec[i].append(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "56200"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(words_vec[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "words_vec = np.asarray(words_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function BufferedWriter.close>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# pickle에 저장하기\n",
    "# test0515 : 5차원 테스트용\n",
    "# test0516 : 100차원 발표 결과용\n",
    "with open('test0516.txt', 'wb') as f:\n",
    "    pickle.dump(words_vec, f)\n",
    "f.close"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function BufferedWriter.close>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# pickle에 저장하기\n",
    "# para0515 : 5차원 테스트\n",
    "# para0516 : 100차원 발표 결과용\n",
    "with open('para0516.txt', 'wb') as f:\n",
    "    pickle.dump(para, f)\n",
    "f.close"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0523"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## pickle load 하기\n",
    "with open('Keyword0523.txt', 'rb') as f:\n",
    "    words = pickle.load(f)\n",
    "f.close()\n",
    "    \n",
    "## pickle load 하기\n",
    "with open('para0523.txt', 'rb') as f:\n",
    "    para = pickle.load(f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['시공', '대회', '가치관', '건물', '건축', '투영', '경진', '중학', '경지', '건설', '일용직', '놀라움', '충남', '시절', '은사', '무임승차', '현장', '벽돌', '인생', '하자']]\n",
      "[\"‘무임승차하지 않는 삶’ 중학 시절, 은사님께서 해주신 말씀입니다. 직접 노력해서 결과를 얻어야 한다는 기본적 원칙은 곧 제 인생의 가치관이 되었습니다. 인생의 가치관이 생기자 새로운 목표가 생겼습니다. 그 계기는 고등학생 시절 학창시절 우연히 하게 된 일용직 근로자 경험에서 시작되었습니다. 작은 벽돌이 쌓여서 큰 건물이 완성되는 모습은 저에겐 큰 놀라움이었습니다. 당시 어린 눈으로 봤던 건설현장은 저의 가치관이 그대로 투영된 곳으로 다가왔고 곧, 건설현장은 저에게 목표가 되었습니다. 저는 건축엔지니어로서의 기본을 다지기 위해 아래와 같은 노력을 했습니다. 다양한 분야의 지식을 습득했습니다. 건축은 구조, 시공, 안전 등 다양한 분야로 이루어져 있습니다. 그 때문에 한 가지 분야만 능통해서는 온전한 건물을 지을 수 없습니다. 그래서 저는 설계, 안전, 환경, 시공 등 다양한 전공수업을 이수하였습니다. 다양한 변수가 있는 현장에서 습득한 지식을 토대로 공기단축에 이바지하도록 하겠습니다. 다양한 경험을 위해 경진대회에 참여했습니다. 학부 시절 시공경진 대회에 참여했습니다. 대회를 준비하는 과정에서의 공부는 시공과 하자에 대한 다양한 지식을 얻을 수 있었습니다. 또한, 21개 팀이 참가한 경진대회에서 100명이 넘는 인원과 충남지역 대기업 실무자분들 앞에서 발표한 경험은 저를 더욱 단단하게 만들어 주었고, '대상'이라는 성과도 얻을 수 있었습니다.\"]\n"
     ]
    }
   ],
   "source": [
    "print(words[0:1])\n",
    "print(para[0:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "1. 추출 단어가 30개 미만, 200개 초과인거 삭제\n",
    "2. 모든 명사를 추출 및 확인\n",
    "3. word2vec을 통해서 벡터화 200차원\n",
    "4. padding을 통해서 맞춰주기 → 함수 만들기\n",
    "5. 모델 돌리기\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/venv/lib/python3.6/site-packages/ipykernel_launcher.py:54: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "## 임베딩 벡터 만들기 ## \n",
    "데이터 전처리\n",
    "words : 명사 추출 데이터\n",
    "para : 문항 데이터\n",
    "para_length : 문항의 최소 글자 수\n",
    "\"\"\"\n",
    "\"\"\"\n",
    "[0523 keyword, paragraph 파일]\n",
    "추출 단어가 30개 미만, 200개 초과인거 삭제\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "이 부분은 이미 진행되었으므로 skip\n",
    "para_length = 400\n",
    "\n",
    "index_over = []\n",
    "for i in range(0, len(para)):\n",
    "    if len(para[i]) < para_length:\n",
    "        index_over.append(i)\n",
    "    \n",
    "index_over.reverse() # 역순으로\n",
    "    \n",
    "# index_over 값을 데이터에서 제거\n",
    "for i in index_over:\n",
    "    del words[i], para[i]\n",
    "\n",
    "\"\"\"    \n",
    "# padding을 위해서(나머지 자리 0으로 채워주기 위해서) 단어의 최고 자리 구하기\n",
    "# words 추출된 단어 개수 확인\n",
    "words_count = []\n",
    "for i in range(0,len(words)):\n",
    "    words_count.append(len(words[i]))\n",
    "    \n",
    "words_length_max = max(words_count) # max_length 설정\n",
    "    \n",
    "# Words Word2Vec\n",
    "embedding_model = Word2Vec(words,\n",
    "                           size = 200,  # 200차원의 벡터로 바꾼다\n",
    "                           window = 2,  # 주변 단어 앞 뒤로 두개까지\n",
    "                           min_count = 1,  # 출현 빈도의 최소값, 나머진 삭제\n",
    "                           workers = 4,  # CPU는 쿼드코어를 사용\n",
    "                           iter = 100, # 100번 학습\n",
    "                           sg = 1 # 분석방법론은 CBOW와 Skip-Gram 중 후자 선택\n",
    "                          )    \n",
    "\n",
    "\n",
    "# 단어를 벡터 값으로 바꾸기\n",
    "words_vec = []\n",
    "for i in range(0, len(words)):\n",
    "    words_vec_before = []\n",
    "    \n",
    "    for j in range(0, len(words[i])):\n",
    "        a = embedding_model[words[i][j]]\n",
    "        for k in range(0, len(a)):\n",
    "            words_vec_before.append(a[k])\n",
    "                \n",
    "    words_vec.append(words_vec_before)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17090\n",
      "4000\n"
     ]
    }
   ],
   "source": [
    "print(len(words_vec))\n",
    "print(len(words_vec[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "dense vector shape fittning\n",
    "벡터의 shape 맞추기! = padding : 0으로 채우자\n",
    "\"\"\"\n",
    "words_vec_length_count = []\n",
    "for i in range(0, len(words_vec)):\n",
    "    words_vec_length_count.append(len(words_vec[i]))\n",
    "\n",
    "vec_max_length = max(words_vec_length_count) # \n",
    "\n",
    "for i in range(0, len(words_vec)):\n",
    "    if len(words_vec[i]) != vec_max_length:\n",
    "        for j in range(0, (vec_max_length-len(words_vec[i]))):\n",
    "            words_vec[i].append(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4000"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(words_vec[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "words_vec = np.asarray(words_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# pickle에 저장하기\n",
    "# test0515 : 5차원 테스트용\n",
    "# test0516 : 100차원 발표 결과용\n",
    "# test0523 : 200차원 면담 결과용\n",
    "with open('test0523.txt', 'wb') as f:\n",
    "    pickle.dump(words_vec, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 영어 자소서 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('para0523.txt', 'rb') as f:\n",
    "    para = np.array(pickle.load(f))\n",
    "        \n",
    "with open('test0523.txt', 'rb') as f:\n",
    "    test = np.array(pickle.load(f))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "인덱스 개수 :  42\n",
      "변경 전 :  17090\n",
      "변경 전 :  17090\n",
      "변경 후 :  17048\n",
      "변경 후 :  17048\n"
     ]
    }
   ],
   "source": [
    "index = []\n",
    "for i in range(0, len(para)):\n",
    "    if ((\"a\" in para[i]) & (\"b\" in para[i]) & (\"c\" in para[i]) & (\"d\" in para[i]) & (\"e\" in para[i]) & (\"f\" in para[i]) & (\"g\" in para[i]) & (\"h\" in para[i]) & (\"i\" in para[i]) ):\n",
    "        index.append(i)\n",
    "\n",
    "print(\"인덱스 개수 : \",len(index))\n",
    "print(\"변경 전 : \", len(para))\n",
    "print(\"변경 전 : \",len(test))\n",
    "\n",
    "# list로 바꿔서 해당 인덱스 삭제\n",
    "para = para.tolist()\n",
    "test = test.tolist()\n",
    "index.reverse()\n",
    "for i in index:\n",
    "    del para[i]\n",
    "    del test[i]\n",
    "    \n",
    "print(\"변경 후 : \", len(para))\n",
    "print(\"변경 후 : \",len(test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('test0528.txt', 'wb') as f:\n",
    "    pickle.dump(test, f)\n",
    "    \n",
    "with open('para0528.txt', 'wb') as f:\n",
    "    pickle.dump(para, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
